{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Stand-Up_Comedian_CharRNN.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "dStJrRgZKfxt",
        "oGwOTnkqKfxx",
        "vmQL_jruKfx0",
        "eT9bW1UxKfx1"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22P34JMuKfxl"
      },
      "source": [
        "# Training a Char RNN to Tell Jokes\n",
        "\n",
        "**Objective**: Build text generation model to be able to complete a sentence for different standup comedians\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfLu6oiIKfxn"
      },
      "source": [
        "## Data Gathering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvUPzaq_Kfxo"
      },
      "source": [
        "import requests               # a simple HTTP library for Python\n",
        "from bs4 import BeautifulSoup # a great library for webscrapping\n",
        "import pickle    "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-9QIgwNKfxo"
      },
      "source": [
        "def url_to_transcript(url):\n",
        "    '''Returns transcript data specifically from scrapsfromtheloft.com.'''\n",
        "    page = requests.get(url).text\n",
        "    soup = BeautifulSoup(page, \"lxml\")\n",
        "    text = [p.text for p in soup.find(class_=\"elementor-element elementor-element-74af9a5b elementor-widget elementor-widget-theme-post-content\").find_all('p')]\n",
        "    print(url)\n",
        "    return text\n",
        "\n",
        "# URLs of transcripts in scope\n",
        "urls = ['http://scrapsfromtheloft.com/2017/05/06/louis-ck-oh-my-god-full-transcript/',\n",
        "        'http://scrapsfromtheloft.com/2017/04/11/dave-chappelle-age-spin-2017-full-transcript/',\n",
        "        'http://scrapsfromtheloft.com/2018/03/15/ricky-gervais-humanity-transcript/',\n",
        "        'http://scrapsfromtheloft.com/2017/08/07/bo-burnham-2013-full-transcript/',\n",
        "        'http://scrapsfromtheloft.com/2017/05/24/bill-burr-im-sorry-feel-way-2014-full-transcript/',\n",
        "        'http://scrapsfromtheloft.com/2017/04/21/jim-jefferies-bare-2014-full-transcript/',\n",
        "        'http://scrapsfromtheloft.com/2017/08/02/john-mulaney-comeback-kid-2015-full-transcript/',\n",
        "        'http://scrapsfromtheloft.com/2017/10/21/hasan-minhaj-homecoming-king-2017-full-transcript/',\n",
        "        'http://scrapsfromtheloft.com/2017/09/19/ali-wong-baby-cobra-2016-full-transcript/',\n",
        "        'http://scrapsfromtheloft.com/2017/08/03/anthony-jeselnik-thoughts-prayers-2015-full-transcript/',\n",
        "        'http://scrapsfromtheloft.com/2018/03/03/mike-birbiglia-my-girlfriends-boyfriend-2013-full-transcript/',\n",
        "        'http://scrapsfromtheloft.com/2017/08/19/joe-rogan-triggered-2016-full-transcript/']\n",
        "\n",
        "# Comedian names\n",
        "comedians = ['louis', 'dave', 'ricky', 'bo', 'bill', 'jim', 'john', 'hasan', 'ali', 'anthony', 'mike', 'joe']"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdOMuTwPKfxp",
        "outputId": "9e42726f-e229-4085-e2b9-0e63972be194"
      },
      "source": [
        "# Requesting transcripts\n",
        "transcripts = [url_to_transcript(u) for u in urls]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "http://scrapsfromtheloft.com/2017/05/06/louis-ck-oh-my-god-full-transcript/\n",
            "http://scrapsfromtheloft.com/2017/04/11/dave-chappelle-age-spin-2017-full-transcript/\n",
            "http://scrapsfromtheloft.com/2018/03/15/ricky-gervais-humanity-transcript/\n",
            "http://scrapsfromtheloft.com/2017/08/07/bo-burnham-2013-full-transcript/\n",
            "http://scrapsfromtheloft.com/2017/05/24/bill-burr-im-sorry-feel-way-2014-full-transcript/\n",
            "http://scrapsfromtheloft.com/2017/04/21/jim-jefferies-bare-2014-full-transcript/\n",
            "http://scrapsfromtheloft.com/2017/08/02/john-mulaney-comeback-kid-2015-full-transcript/\n",
            "http://scrapsfromtheloft.com/2017/10/21/hasan-minhaj-homecoming-king-2017-full-transcript/\n",
            "http://scrapsfromtheloft.com/2017/09/19/ali-wong-baby-cobra-2016-full-transcript/\n",
            "http://scrapsfromtheloft.com/2017/08/03/anthony-jeselnik-thoughts-prayers-2015-full-transcript/\n",
            "http://scrapsfromtheloft.com/2018/03/03/mike-birbiglia-my-girlfriends-boyfriend-2013-full-transcript/\n",
            "http://scrapsfromtheloft.com/2017/08/19/joe-rogan-triggered-2016-full-transcript/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwhskpegKfxq"
      },
      "source": [
        "!mkdir transcripts\n",
        "\n",
        "for i, c in enumerate(comedians):\n",
        "    with open(\"transcripts/\" + c + \".txt\", \"wb\") as file:\n",
        "        pickle.dump(transcripts[i], file)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKOTHY1uKfxq"
      },
      "source": [
        "# Load pickle file\n",
        "data = {}\n",
        "for i, c in enumerate(comedians):\n",
        "    with open(\"transcripts/\" + c + \".txt\", \"rb\") as file:\n",
        "        data[c] = pickle.load(file)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ugZAh8oKfxr",
        "outputId": "bde8782a-ce72-44d5-a4d7-a4812eb59875"
      },
      "source": [
        "# Take a look at the data\n",
        "data.keys() ,data['louis'][:1]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(dict_keys(['louis', 'dave', 'ricky', 'bo', 'bill', 'jim', 'john', 'hasan', 'ali', 'anthony', 'mike', 'joe']),\n",
              " ['Intro\\nFade the music out. Let’s roll. Hold there. Lights. Do the lights. Thank you. Thank you very much. I appreciate that. I don’t necessarily agree with you, but I appreciate very much. Well, this is a nice place. This is easily the nicest place For many miles in every direction. That’s how you compliment a building And shit on a town with one sentence. It is odd around here, as I was driving here. There doesn’t seem to be any difference Between the sidewalk and the street for pedestrians here. People just kind of walk in the middle of the road. I love traveling And seeing all the different parts of the country. I live in New York. I live in a– There’s no value to your doing that at all.'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jfgKMUdKfxr"
      },
      "source": [
        "## Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7K3ShTTlKfxr"
      },
      "source": [
        "def combine_text(list_of_text):\n",
        "    '''\n",
        "    Take a list of texts and combine them into one large chunk of text\n",
        "    \n",
        "    Return a text (larger one)\n",
        "    '''\n",
        "    return ' '.join(list_of_text)\n",
        "    "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSwq9XmuKfxs"
      },
      "source": [
        "# Combining\n",
        "data_combined = {key: [combine_text(value)] for key, value in data.items()}"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "tV9gd8d9Kfxs",
        "outputId": "f2fd5278-b4fc-431a-d62d-53f8832aa11a"
      },
      "source": [
        "import pandas as pd\n",
        "pd.set_option('max_colwidth', 150)\n",
        "\n",
        "data_df = pd.DataFrame.from_dict(data_combined).transpose()\n",
        "data_df.columns = ['transcript']\n",
        "data_df = data_df.sort_index()\n",
        "data_df.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>transcript</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ali</th>\n",
              "      <td>Ladies and gentlemen, please welcome to the stage: Ali Wong! Hi. Hello! Welcome! Thank you! Thank you for coming. Hello! Hello. We are gonna have ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>anthony</th>\n",
              "      <td>Thank you. Thank you. Thank you, San Francisco. Thank you so much. So good to be here. People were surprised when I told ’em I was gonna tape my s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bill</th>\n",
              "      <td>[cheers and applause] All right, thank you! Thank you very much! Thank you. Thank you. Thank you. How are you? What’s going on? Thank you. It’s a ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bo</th>\n",
              "      <td>Bo What? Old MacDonald had a farm E I E I O And on that farm he had a pig E I E I O Here a snort There a Old MacDonald had a farm E I E I O [Appla...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dave</th>\n",
              "      <td>This is Dave. He tells dirty jokes for a living. That stare is where most of his hard work happens. It signifies a profound train of thought, the ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                    transcript\n",
              "ali      Ladies and gentlemen, please welcome to the stage: Ali Wong! Hi. Hello! Welcome! Thank you! Thank you for coming. Hello! Hello. We are gonna have ...\n",
              "anthony  Thank you. Thank you. Thank you, San Francisco. Thank you so much. So good to be here. People were surprised when I told ’em I was gonna tape my s...\n",
              "bill     [cheers and applause] All right, thank you! Thank you very much! Thank you. Thank you. Thank you. How are you? What’s going on? Thank you. It’s a ...\n",
              "bo       Bo What? Old MacDonald had a farm E I E I O And on that farm he had a pig E I E I O Here a snort There a Old MacDonald had a farm E I E I O [Appla...\n",
              "dave     This is Dave. He tells dirty jokes for a living. That stare is where most of his hard work happens. It signifies a profound train of thought, the ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwkERvvnnw53"
      },
      "source": [
        "data_dave_unclean = data_df.iloc[4].values"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMSCBz2a6p5f",
        "outputId": "08411a8c-f260-43eb-ba77-387326604059"
      },
      "source": [
        "len(data_dave_unclean[0])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "48194"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "GhHJg5_tKfxs",
        "outputId": "c3a99f7f-58a1-4377-cbd5-1f93497d83c2"
      },
      "source": [
        "import re   \n",
        "import string\n",
        "\n",
        "def clean_text_re(text):\n",
        "    '''Make text lowercase, remove text in square brackets, remove punctuation,\n",
        "       remove words containing numbers, additional punctuation and non-sensical text.'''\n",
        "    text = text.lower()\n",
        "    text = re.sub('\\[.*?\\]', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    #text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    text = re.sub('[‘’“”…]', '', text)\n",
        "    text = re.sub('\\n', '', text)\n",
        "    return text\n",
        "\n",
        "step1 = lambda x: clean_text_re(x)\n",
        "\n",
        "# Let clean the data:\n",
        "data_clean = pd.DataFrame(data_df.transcript.apply(step1))\n",
        "data_df.to_pickle(\"corpus.pkl\")\n",
        "print('DATA after cleaning with regular expression')\n",
        "data_clean.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DATA after cleaning with regular expression\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>transcript</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ali</th>\n",
              "      <td>ladies and gentlemen please welcome to the stage ali wong hi hello welcome thank you thank you for coming hello hello we are gonna have to get thi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>anthony</th>\n",
              "      <td>thank you thank you thank you san francisco thank you so much so good to be here people were surprised when i told em i was gonna tape my special ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bill</th>\n",
              "      <td>all right thank you thank you very much thank you thank you thank you how are you whats going on thank you its a pleasure to be here in the great...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bo</th>\n",
              "      <td>bo what old macdonald had a farm e i e i o and on that farm he had a pig e i e i o here a snort there a old macdonald had a farm e i e i o  this i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dave</th>\n",
              "      <td>this is dave he tells dirty jokes for a living that stare is where most of his hard work happens it signifies a profound train of thought the alch...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                    transcript\n",
              "ali      ladies and gentlemen please welcome to the stage ali wong hi hello welcome thank you thank you for coming hello hello we are gonna have to get thi...\n",
              "anthony  thank you thank you thank you san francisco thank you so much so good to be here people were surprised when i told em i was gonna tape my special ...\n",
              "bill      all right thank you thank you very much thank you thank you thank you how are you whats going on thank you its a pleasure to be here in the great...\n",
              "bo       bo what old macdonald had a farm e i e i o and on that farm he had a pig e i e i o here a snort there a old macdonald had a farm e i e i o  this i...\n",
              "dave     this is dave he tells dirty jokes for a living that stare is where most of his hard work happens it signifies a profound train of thought the alch..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwc9PmIHvFHu"
      },
      "source": [
        "data_clean.to_csv('comedian_transcripts.csv')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVaQbUTsu8y1"
      },
      "source": [
        "ali_df = data_clean.iloc[0].values\n",
        "anthony_df = data_clean.iloc[1].values\n",
        "bill_df = data_clean.iloc[2].values\n",
        "bo_df = data_clean.iloc[3].values\n",
        "dave_df = data_clean.iloc[4].values\n",
        "hasan_df = data_clean.iloc[5].values\n",
        "jim_df = data_clean.iloc[6].values\n",
        "joe_df = data_clean.iloc[7].values\n",
        "john_df = data_clean.iloc[8].values\n",
        "louis_df = data_clean.iloc[9].values\n",
        "mike_df = data_clean.iloc[10].values\n",
        "ricky_df = data_clean.iloc[11].values"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnFjP8BStelc"
      },
      "source": [
        "ali_df_unclean = data_df.iloc[0].values\n",
        "anthony_df_unclean = data_df.iloc[1].values\n",
        "bill_df_unclean = data_df.iloc[2].values\n",
        "bo_df_unclean = data_df.iloc[3].values\n",
        "dave_df_unclean = data_df.iloc[4].values\n",
        "hasan_df_unclean = data_df.iloc[5].values\n",
        "jim_df_unclean = data_df.iloc[6].values\n",
        "joe_df_unclean = data_df.iloc[7].values\n",
        "john_df_unclean = data_df.iloc[8].values\n",
        "louis_df_unclean = data_df.iloc[9].values\n",
        "mike_df_unclean = data_df.iloc[10].values\n",
        "ricky_df_unclean = data_df.iloc[11].values"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYiwHOfT6we3",
        "outputId": "a6742e0c-2773-451c-8274-8a9321355d7c"
      },
      "source": [
        "len(hasan_df_unclean[0])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "56094"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmwk8qJzvTv4"
      },
      "source": [
        "combined_comedian = ali_df[0]+anthony_df[0]+bill_df[0]+bo_df+dave_df+hasan_df+jim_df+joe_df+john_df+louis_df+mike_df+ricky_df"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ds9dwjEwvb5m"
      },
      "source": [
        "textfile_combined = open('combined_script.txt','w')\n",
        "textfile_combined.write(combined_comedian[0])\n",
        "textfile_combined.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoGdyNhBKmps"
      },
      "source": [
        "## TextGenRnn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBjvL5-VKr4z",
        "outputId": "965c58ac-874a-4c14-ae73-ff7f5e610ca3"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqhZuAEoLVuD",
        "outputId": "5c411903-aaef-4e81-f0b5-5fd632c384a9"
      },
      "source": [
        "!pip3 install git+git://github.com/minimaxir/textgenrnn.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/minimaxir/textgenrnn.git\n",
            "  Cloning git://github.com/minimaxir/textgenrnn.git to /tmp/pip-req-build-yrbv_m6c\n",
            "  Running command git clone -q git://github.com/minimaxir/textgenrnn.git /tmp/pip-req-build-yrbv_m6c\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from textgenrnn==2.0.0) (3.1.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from textgenrnn==2.0.0) (0.22.2.post1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from textgenrnn==2.0.0) (4.41.1)\n",
            "Collecting tensorflow>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/aa/fd/993aa1333eb54d9f000863fe8ec61e41d12eb833dea51484c76c038718b5/tensorflow-2.5.0-cp37-cp37m-manylinux2010_x86_64.whl (454.3MB)\n",
            "\u001b[K     |████████████████████████████████| 454.3MB 40kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.5; python_version == \"3.7\" in /usr/local/lib/python3.7/dist-packages (from h5py->textgenrnn==2.0.0) (1.19.5)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py->textgenrnn==2.0.0) (1.5.2)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->textgenrnn==2.0.0) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->textgenrnn==2.0.0) (1.0.1)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->textgenrnn==2.0.0) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->textgenrnn==2.0.0) (3.7.4.3)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->textgenrnn==2.0.0) (0.2.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->textgenrnn==2.0.0) (0.12.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->textgenrnn==2.0.0) (1.1.2)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->textgenrnn==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->textgenrnn==2.0.0) (3.3.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->textgenrnn==2.0.0) (0.4.0)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->textgenrnn==2.0.0) (1.34.1)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->textgenrnn==2.0.0) (3.12.4)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->textgenrnn==2.0.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->textgenrnn==2.0.0) (1.12)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->textgenrnn==2.0.0) (2.5.0.dev2021032900)\n",
            "Collecting tensorflow-estimator<2.6.0,>=2.5.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/78/b27f73e923becc6e79e18fe112cf75e3200d1ee35b0dba8fa46181bce56c/tensorflow_estimator-2.5.0-py2.py3-none-any.whl (462kB)\n",
            "\u001b[K     |████████████████████████████████| 471kB 43.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->textgenrnn==2.0.0) (1.12.1)\n",
            "Collecting tensorboard~=2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/44/f5/7feea02a3fb54d5db827ac4b822a7ba8933826b36de21880518250b8733a/tensorboard-2.5.0-py3-none-any.whl (6.0MB)\n",
            "\u001b[K     |████████████████████████████████| 6.0MB 38.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->textgenrnn==2.0.0) (0.36.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.9.2->tensorflow>=2.1.0->textgenrnn==2.0.0) (57.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.1.0->textgenrnn==2.0.0) (2.23.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.1.0->textgenrnn==2.0.0) (1.30.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.1.0->textgenrnn==2.0.0) (0.4.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.1.0->textgenrnn==2.0.0) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.1.0->textgenrnn==2.0.0) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.1.0->textgenrnn==2.0.0) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.1.0->textgenrnn==2.0.0) (1.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow>=2.1.0->textgenrnn==2.0.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow>=2.1.0->textgenrnn==2.0.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow>=2.1.0->textgenrnn==2.0.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow>=2.1.0->textgenrnn==2.0.0) (2020.12.5)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow>=2.1.0->textgenrnn==2.0.0) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow>=2.1.0->textgenrnn==2.0.0) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow>=2.1.0->textgenrnn==2.0.0) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow>=2.1.0->textgenrnn==2.0.0) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow>=2.1.0->textgenrnn==2.0.0) (4.0.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow>=2.1.0->textgenrnn==2.0.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow>=2.1.0->textgenrnn==2.0.0) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.5->tensorflow>=2.1.0->textgenrnn==2.0.0) (3.4.1)\n",
            "Building wheels for collected packages: textgenrnn\n",
            "  Building wheel for textgenrnn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for textgenrnn: filename=textgenrnn-2.0.0-cp37-none-any.whl size=1734434 sha256=8b21060ee4abd98af7c1db5b722d954f40887654002041bb01af19e308708b74\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-00pb8cpj/wheels/51/cd/43/32656d2da914e1bda73eeafa357602d92a9f67b587dfdd5aa8\n",
            "Successfully built textgenrnn\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, tensorflow, textgenrnn\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow 1.15.2\n",
            "    Uninstalling tensorflow-1.15.2:\n",
            "      Successfully uninstalled tensorflow-1.15.2\n",
            "Successfully installed tensorboard-2.5.0 tensorflow-2.5.0 tensorflow-estimator-2.5.0 textgenrnn-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9iEgXx6KwYS",
        "outputId": "f594f7d0-f337-4567-9d18-992f7b5b57a8"
      },
      "source": [
        "from google.colab import files\n",
        "import textgenrnn\n",
        "from datetime import datetime\n",
        "import os\n",
        "import keras"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ah4mJsiab4gM"
      },
      "source": [
        "from textgenrnn import textgenrnn\n",
        "\n",
        "textgen = textgenrnn()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQS4axcbkav9",
        "outputId": "96f7e5e8-88aa-44fe-f766-16bcbafd5b91"
      },
      "source": [
        "print('Input:', ali_df_unclean[0][102:141] )\n",
        "print('Target:', ali_df_unclean[0][141])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: for coming. Hello! Hello. We are gonna \n",
            "Target: h\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCnPAmMgNw15",
        "outputId": "478dc8fc-f627-4763-b463-a0404cff7fc7"
      },
      "source": [
        "textgen_ali = textgenrnn(name = 'aliwong_model')\n",
        "textgen_ali.train_on_texts(ali_df_unclean,\n",
        "                            rnn_bidirectional = True,\n",
        "                            rnn_size = 128,\n",
        "                            rnn_layers = 3,\n",
        "                            max_length = 40,\n",
        "                            dim_embeddings = 300,\n",
        "                            num_epochs = 15,\n",
        "                            gen_epochs = 15)\n",
        "\n",
        "print(textgen_ali.model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on 38,969 character sequences.\n",
            "Epoch 1/15\n",
            "304/304 [==============================] - 39s 98ms/step - loss: 1.4777\n",
            "Epoch 2/15\n",
            "304/304 [==============================] - 31s 101ms/step - loss: 1.2804\n",
            "Epoch 3/15\n",
            "304/304 [==============================] - 31s 100ms/step - loss: 1.1670\n",
            "Epoch 4/15\n",
            "304/304 [==============================] - 31s 101ms/step - loss: 1.0882\n",
            "Epoch 5/15\n",
            "304/304 [==============================] - 30s 100ms/step - loss: 1.0177\n",
            "Epoch 6/15\n",
            "304/304 [==============================] - 31s 101ms/step - loss: 0.9525\n",
            "Epoch 7/15\n",
            "304/304 [==============================] - 31s 101ms/step - loss: 0.9020\n",
            "Epoch 8/15\n",
            "304/304 [==============================] - 31s 101ms/step - loss: 0.8413\n",
            "Epoch 9/15\n",
            "304/304 [==============================] - 31s 101ms/step - loss: 0.7817\n",
            "Epoch 10/15\n",
            "304/304 [==============================] - 31s 101ms/step - loss: 0.7242\n",
            "Epoch 11/15\n",
            "304/304 [==============================] - 31s 100ms/step - loss: 0.6687\n",
            "Epoch 12/15\n",
            "304/304 [==============================] - 30s 100ms/step - loss: 0.6163\n",
            "Epoch 13/15\n",
            "304/304 [==============================] - 30s 100ms/step - loss: 0.5681\n",
            "Epoch 14/15\n",
            "304/304 [==============================] - 30s 100ms/step - loss: 0.5232\n",
            "Epoch 15/15\n",
            "304/304 [==============================] - 31s 100ms/step - loss: 0.4862\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "Ladies and moment with a fucking front percent of your life. That’s what he does. “Oh, my God. I don’t know what that says that you don’t gotta shit you feel very scared on the shit. They don’t gotta use that the baby created miscarriage. And then, a lot of people are you about that that the turn i\n",
            "\n",
            "Ladies and scared of comics. I’m half-Chinese and just wants me anymore. Because I’m a little bit… …that has has it and a half months pregnant. And you know, a lot of my friends, were shit in the back of the pain. They don’t gotta use the time, that he was like, “Oh, my God, who is that?” And then,\n",
            "\n",
            "Ladies and some guarantels and I lost in the fucking bracey as a secret paper to put it out of the pain. When I want to take their eyes and she was like, “Oh, my God, who is the butt off and have a lot of women who are the toilet people. We don’t wanna work anymore. I’m not think of your husband wa\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "Ladies and mainten, process, and then, a lot of women who are Chinese last year. It’s dupher, that shit is too tilm, you know that you’re into the pain. When I see and then it because they be a lot of “The Craight day you snouch with the country and be like, “I have to purpose, being a man who are \n",
            "\n",
            "Ladies and mom– Look at  You’re the Buddha women who think that shit there is any grew out of the pain. When I was like, “Yeah, that’s the bungie or such stimulated and then it taught has to tell him and if you’re a kid because the bad helming as then we blow about an up bitch shit around to making\n",
            "\n",
            "Ladies and page, the case in the baby for the rest of my personal. We like to do it. It’s because I was like, “She’s burn when I can bling a bad day” and then we half metabolism, that you don’t know what that do in your white word “bossy” in the women who are the baby for fireful investment. Housew\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "Gpuaran Krusef-- The baby looks like people are OK? Maybe out all off me.” “Oh, are you, yeah, I did is gonna have exerss is dunked if you want.You know, i work at italial for the park. I want you, in a cowrelor people. I don’t gotta shit up if I saw you, a hugy weddie sponsomeing. So, that’s lives\n",
            "\n",
            "Ladies of spanks for Volvo. I imanec a fear. That’s what happens where they can explain she did he she’s gonna happening to be pregnant, and a shitty day all the puble daylant. Please! Shut basical Instead when the home called me jealt that this part of lead of chicken shit arounant, because arthem\n",
            "\n",
            "Ladies, my Guardian, how no one have to calm all the woman that, and find of interesting, you would just rehrentate your husband and was the channel on Her. On the alabamas, the exprint is as there, and I told me any given my husband, ͅtugge white husband don’t feel smelp me to see how Uh, point no\n",
            "\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input (InputLayer)              [(None, 40)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 40, 100)      46500       input[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "rnn_1 (LSTM)                    (None, 40, 128)      117248      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "rnn_2 (LSTM)                    (None, 40, 128)      131584      rnn_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "rnn_concat (Concatenate)        (None, 40, 356)      0           embedding[0][0]                  \n",
            "                                                                 rnn_1[0][0]                      \n",
            "                                                                 rnn_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "attention (AttentionWeightedAve (None, 356)          356         rnn_concat[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "output (Dense)                  (None, 465)          166005      attention[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 461,693\n",
            "Trainable params: 461,693\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wwFavXEJ61z",
        "outputId": "5a3e7f21-5327-4d72-f4e6-0d049e2a3674"
      },
      "source": [
        "textgen_ali.generate(1, prefix = 'My husband', max_gen_length=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "My husband at home six-week Chinese every dad. And it was like, “But… I don’t wanna die!” You wanna\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LU3BhCWp8WB",
        "outputId": "11a88b11-2b25-4603-89a2-99dd068e5b66"
      },
      "source": [
        "textgen_Dave = textgenrnn(name = 'davechappelle_model')\n",
        "textgen_Dave.train_on_texts(data_dave_unclean,\n",
        "                            rnn_bidirectional = True,\n",
        "                            rnn_layers = 3,\n",
        "                            max_length = 40,\n",
        "                            rnn_size = 128,\n",
        "                            dim_embeddings = 300,\n",
        "                            num_epochs = 20,\n",
        "                            gen_epochs = 20)\n",
        "\n",
        "print(textgen_Dave.model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training new model w/ 3-layer, 128-cell Bidirectional LSTMs\n",
            "Training on 48,195 character sequences.\n",
            "Epoch 1/20\n",
            "376/376 [==============================] - 56s 132ms/step - loss: 2.5085\n",
            "Epoch 2/20\n",
            "376/376 [==============================] - 49s 131ms/step - loss: 1.8703\n",
            "Epoch 3/20\n",
            "376/376 [==============================] - 50s 132ms/step - loss: 1.6569\n",
            "Epoch 4/20\n",
            "376/376 [==============================] - 49s 129ms/step - loss: 1.5097\n",
            "Epoch 5/20\n",
            "376/376 [==============================] - 48s 128ms/step - loss: 1.3949\n",
            "Epoch 6/20\n",
            "376/376 [==============================] - 48s 128ms/step - loss: 1.2943\n",
            "Epoch 7/20\n",
            "376/376 [==============================] - 48s 128ms/step - loss: 1.1977\n",
            "Epoch 8/20\n",
            "376/376 [==============================] - 48s 128ms/step - loss: 1.1014\n",
            "Epoch 9/20\n",
            "376/376 [==============================] - 48s 128ms/step - loss: 1.0026\n",
            "Epoch 10/20\n",
            "376/376 [==============================] - 48s 129ms/step - loss: 0.9033\n",
            "Epoch 11/20\n",
            "376/376 [==============================] - 48s 128ms/step - loss: 0.8062\n",
            "Epoch 12/20\n",
            "376/376 [==============================] - 48s 127ms/step - loss: 0.7044\n",
            "Epoch 13/20\n",
            "376/376 [==============================] - 48s 128ms/step - loss: 0.6078\n",
            "Epoch 14/20\n",
            "376/376 [==============================] - 48s 129ms/step - loss: 0.5077\n",
            "Epoch 15/20\n",
            "376/376 [==============================] - 48s 128ms/step - loss: 0.4224\n",
            "Epoch 16/20\n",
            "376/376 [==============================] - 48s 128ms/step - loss: 0.3537\n",
            "Epoch 17/20\n",
            "376/376 [==============================] - 49s 130ms/step - loss: 0.2773\n",
            "Epoch 18/20\n",
            "376/376 [==============================] - 48s 129ms/step - loss: 0.2180\n",
            "Epoch 19/20\n",
            "376/376 [==============================] - 49s 129ms/step - loss: 0.1727\n",
            "Epoch 20/20\n",
            "376/376 [==============================] - 48s 128ms/step - loss: 0.1402\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "This is Dave. He tells dirty jokes for a living. That stare is where most of his hard work happens. I can’t keep track of all this shit.” And then my son pushed passed me. It was so bad, the teacher looked at all the way back at me and says, “Dad, please. Please, I have to meet him.” I was like, “H\n",
            "\n",
            "This is Dave. He tells dirty jokes for a living. That stare is where most of his hard work happens. I can’t keep track of all this shit. So you just give the fuck up. That’s right. You remember. It fucking exploded! Right on television. Everybody’s mad at police now. I watched that would be like if\n",
            "\n",
            "This is Dave. He tells dirty jokes for a living. That stare is where most of his hard work happens. I can’t keep track of all this shit.” And then my son pushed passed me. It was so bad, the teacher looked at all the way back at me and says, “Dad, please. Please, I have to meet him.” I was like, “H\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "This is Dave. He tells dirty jokes for a living. That stare is where most of his hard work happens. I can’t keep track and field. We’d never met Bill Cosby’s allediated way bear people. They were in Europe– just to be real numbers. He’s like the garments That we wear? ♪\n",
            " ♪ I instruct you to be on t\n",
            "\n",
            "This is Dave. He tells dirty jokes for a living. That motherfucker was up to the radio when someone else is outside of the car. But then, when I looked back at my agents, and we’re seen the first time, in a nutshell. It’s for their faces to switch up like, “What the fuck up. That’s all it would’ve \n",
            "\n",
            "This is Dave. He tells dirty jokes for a living. That stare is where most of his hard work happens. I can’t even know what I mean? How the fuck are you gonna take the show– thousands of dollars– I said, “I know.” And this is when she went too far: “We suffer just came out. “OJ!” He stopped, turned \n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "This is Dave. He tells dirty jokes for a hero idear me. He said, “If you like, that shit why I got over themselves as for saves everybody’s came after and everything would think? Does she think that I don’t have a sneaker deal, we had a very good looked at my friend saw me. I’m sorry, ladies and ge\n",
            "\n",
            "This is Dave. He tells dirty jokes for a Care a few reason happens. I can’t believe that shit out the nicest men?! You can believe you know the same. American women no. Dave thing on right now, Phiftly freh up! We’re gonna do about this bombdy that I’m saying? This is this is a terrifying sce doesn\n",
            "\n",
            "This is Dave. He tells dirty in the problem– and Pven to the with Kevin, too, too, through the American just give the Vewerses and gentlemen, man famh. You get a putting up, but and that even belies and be he do band Wal ween otherne wheir hosculb this movement. But I made the houng menters– I say \n",
            "\n",
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input (InputLayer)              [(None, 40)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 40, 300)      24600       input[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "rnn_1 (Bidirectional)           (None, 40, 256)      439296      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "rnn_2 (Bidirectional)           (None, 40, 256)      394240      rnn_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "rnn_3 (Bidirectional)           (None, 40, 256)      394240      rnn_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "rnn_concat (Concatenate)        (None, 40, 1068)     0           embedding[0][0]                  \n",
            "                                                                 rnn_1[0][0]                      \n",
            "                                                                 rnn_2[0][0]                      \n",
            "                                                                 rnn_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "attention (AttentionWeightedAve (None, 1068)         1068        rnn_concat[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "output (Dense)                  (None, 82)           87658       attention[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 1,341,102\n",
            "Trainable params: 1,341,102\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFeAk-X8wYTG",
        "outputId": "a04262ff-e7bc-4181-d25d-10850d056e61"
      },
      "source": [
        "textgen_bill = textgenrnn(name='billburr_model')\n",
        "textgen_bill.train_on_texts(bill_df_unclean,\n",
        "                            rnn_bidirectional = True,\n",
        "                            rnn_layers = 3,\n",
        "                            max_length = 40,\n",
        "                            rnn_size = 128,\n",
        "                            dim_embeddings = 300,\n",
        "                            num_epochs = 20,\n",
        "                            gen_epochs = 20)\n",
        "\n",
        "print(textgen_bill.model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training new model w/ 3-layer, 128-cell Bidirectional LSTMs\n",
            "Training on 64,608 character sequences.\n",
            "Epoch 1/20\n",
            "504/504 [==============================] - 88s 162ms/step - loss: 2.4993\n",
            "Epoch 2/20\n",
            "504/504 [==============================] - 81s 161ms/step - loss: 1.7908\n",
            "Epoch 3/20\n",
            "504/504 [==============================] - 82s 163ms/step - loss: 1.6191\n",
            "Epoch 4/20\n",
            "504/504 [==============================] - 83s 164ms/step - loss: 1.4974\n",
            "Epoch 5/20\n",
            "504/504 [==============================] - 82s 162ms/step - loss: 1.4005\n",
            "Epoch 6/20\n",
            "504/504 [==============================] - 85s 168ms/step - loss: 1.3161\n",
            "Epoch 7/20\n",
            "504/504 [==============================] - 84s 167ms/step - loss: 1.2321\n",
            "Epoch 8/20\n",
            "504/504 [==============================] - 83s 165ms/step - loss: 1.1520\n",
            "Epoch 9/20\n",
            "504/504 [==============================] - 83s 164ms/step - loss: 1.0682\n",
            "Epoch 10/20\n",
            "297/504 [================>.............] - ETA: 33s - loss: 0.9656"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "LxvmrmBhwmiR",
        "outputId": "7b451a43-f347-42f5-bfe0-f32e45306955"
      },
      "source": [
        "textgen_hasan = textgenrnn(name = 'hasan_model')\n",
        "textgen_hasan.train_on_texts(hasan_df_unclean,\n",
        "                            rnn_bidirectional = True,\n",
        "                            rnn_layers = 3,\n",
        "                            max_length = 40,\n",
        "                            rnn_size = 128,\n",
        "                            dim_embeddings = 300,\n",
        "                            num_epochs = 20,\n",
        "                            gen_epochs = 20)\n",
        "\n",
        "print(textgen_hasan.model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/py_checkpoint_reader.py\u001b[0m in \u001b[0;36mNewCheckpointReader\u001b[0;34m(filepattern)\u001b[0m\n\u001b[1;32m     94\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mCheckpointReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m   \u001b[0;31m# TODO(b/143319754): Remove the RuntimeError casting logic once we resolve the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for hasan_model",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-abc47204fe47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtextgen_hasan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtextgenrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hasan_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m textgen_hasan.train_new_model(hasan_df_unclean,\n\u001b[1;32m      3\u001b[0m                             \u001b[0mrnn_bidirectional\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                             \u001b[0mrnn_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                             \u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/textgenrnn/textgenrnn.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, weights_path, vocab_path, config_path, name, allow_growth)\u001b[0m\n\u001b[1;32m     79\u001b[0m         self.model = textgenrnn_model(self.num_classes,\n\u001b[1;32m     80\u001b[0m                                       \u001b[0mcfg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                                       weights_path=weights_path)\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices_char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/textgenrnn/model.py\u001b[0m in \u001b[0;36mtextgenrnn_model\u001b[0;34m(num_classes, cfg, context_size, weights_path, dropout, optimizer)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mweights_path\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[1;32m   2293\u001b[0m           'True when by_name is True.')\n\u001b[1;32m   2294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2295\u001b[0;31m     \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_detect_save_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2296\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msave_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tf'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2297\u001b[0m       \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trackable_saver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_detect_save_format\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m   2916\u001b[0m   \u001b[0;31m# directory. It's possible for filepath to be both a prefix and directory.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2917\u001b[0m   \u001b[0;31m# Prioritize checkpoint over SavedModel.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2918\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0m_is_readable_tf_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2919\u001b[0m     \u001b[0msave_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'tf'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2920\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0msm_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_is_readable_tf_checkpoint\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m   2937\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_is_readable_tf_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2938\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2939\u001b[0;31m     \u001b[0mpy_checkpoint_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNewCheckpointReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2940\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2941\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLossError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/py_checkpoint_reader.py\u001b[0m in \u001b[0;36mNewCheckpointReader\u001b[0;34m(filepattern)\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0;31m# issue with throwing python exceptions from C++.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0merror_translator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/py_checkpoint_reader.py\u001b[0m in \u001b[0;36merror_translator\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m     33\u001b[0m       \u001b[0;34m'Failed to find any '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m       'matching files for') in error_message:\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0merrors_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m   elif 'Sliced checkpoints are not supported' in error_message or (\n\u001b[1;32m     37\u001b[0m       \u001b[0;34m'Data type '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for hasan_model"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "9JApmJJnwu8E",
        "outputId": "f3a7c4b0-b255-4d29-88bc-c9189b1a884d"
      },
      "source": [
        "textgen_jim = textgenrnn(name = 'jim_model')\n",
        "textgen_jim.train_on_texts(jim_df_unclean,\n",
        "                            rnn_bidirectional = True,\n",
        "                            rnn_layers = 3,\n",
        "                            max_length = 40,\n",
        "                            rnn_size = 128,\n",
        "                            dim_embeddings = 300,\n",
        "                            num_epochs = 20,\n",
        "                            gen_epochs = 20)\n",
        "\n",
        "print(textgen_jim.model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on 58,681 character sequences.\n",
            "Epoch 1/20\n",
            "458/458 [==============================] - 164s 351ms/step - loss: 1.7317\n",
            "Epoch 2/20\n",
            "458/458 [==============================] - 162s 353ms/step - loss: 1.3706\n",
            "Epoch 3/20\n",
            "458/458 [==============================] - 162s 353ms/step - loss: 1.2382\n",
            "Epoch 4/20\n",
            "458/458 [==============================] - 161s 352ms/step - loss: 1.1595\n",
            "Epoch 5/20\n",
            "458/458 [==============================] - 162s 353ms/step - loss: 1.0925\n",
            "Epoch 6/20\n",
            "458/458 [==============================] - 161s 351ms/step - loss: 1.0340\n",
            "Epoch 7/20\n",
            "458/458 [==============================] - 161s 352ms/step - loss: 0.9796\n",
            "Epoch 8/20\n",
            "458/458 [==============================] - 161s 353ms/step - loss: 0.9359\n",
            "Epoch 9/20\n",
            "458/458 [==============================] - 162s 353ms/step - loss: 0.8888\n",
            "Epoch 10/20\n",
            "458/458 [==============================] - 161s 351ms/step - loss: 0.8435\n",
            "Epoch 11/20\n",
            "458/458 [==============================] - 159s 347ms/step - loss: 0.8003\n",
            "Epoch 12/20\n",
            "458/458 [==============================] - 159s 347ms/step - loss: 0.7572\n",
            "Epoch 13/20\n",
            "458/458 [==============================] - 159s 347ms/step - loss: 0.7136\n",
            "Epoch 14/20\n",
            "458/458 [==============================] - 158s 344ms/step - loss: 0.6749\n",
            "Epoch 15/20\n",
            "458/458 [==============================] - 159s 347ms/step - loss: 0.6361\n",
            "Epoch 16/20\n",
            "458/458 [==============================] - 159s 347ms/step - loss: 0.5907\n",
            "Epoch 17/20\n",
            "458/458 [==============================] - 159s 346ms/step - loss: 0.5513\n",
            "Epoch 18/20\n",
            "458/458 [==============================] - 160s 350ms/step - loss: 0.5149\n",
            "Epoch 19/20\n",
            "458/458 [==============================] - 159s 347ms/step - loss: 0.4828\n",
            "Epoch 20/20\n",
            "458/458 [==============================] - 159s 348ms/step - loss: 0.4554\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "[Car horn honks] [Audience cheering] [Announcer] Let he was a man goes to stand out the shopping American that well me it?” And then she went, “Fuck you! Don’t take you!” Then they don’t get a towel. I’m sure they’re the guy’s biight in my dick and all the second there with a man, but when you go t\n",
            "\n",
            "[Car horn honks] [Audience cheering] [Announcer] Let he comes into converson, put you to take it in the ass on Earth. I was in the world, and you go… “That’s why you go up to the counter.” So I want a business class ticket, I don’t know if you wanna know that what you want to do that would be illeg\n",
            "\n",
            "[Car horn honks] [Audience cheering] [Announcer] Let he was a man goes to say that my girlfriend got pregnant to the second that women don’t think they don’t know if you’re not a bad guy and wholly as me. I think you do kill never get a fucking day and as much money as the second there with men tha\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "[Car horn honks] I thonk the model is, the fuck me standing hard. I think they don’t know they all. I have to walk as a fucking cill of course on the stairs was a women that women go and they go to the morning about how all the fucker for comedy up me. I’m like, “What do you want for me?” And I did\n",
            "\n",
            "[Car houng] why it last some    hooker for the morning who would’ve the rape on the boy was like, “Oh, I don’t know if you want that direct guns. And then she went, “Oh, I said, “Hank” And then the many of the short couple was a big party with everyone’s girlfriend. [Grunting] I had a good answer. \n",
            "\n",
            "[Car holbestoring sparing] I think I’m going, “Fucking now done?” Now, we go to their eysy up to the Sandy Hook would act like, “I’m gonna say a man’s badged in the shower. My girlfriend went, “Yeah, I really like here. Why say that right now in their face, and I wanted a lot of conventions, people\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "[Audiuparans over the home hugeles incingul that shit, and I’m invite pircilities. None of them known… I don’t know of them to a hate fathers on Valentine’s Day, which is the real, him. And that’s why Australia don’t know there went, “Fuck you!” Like your girlfriend got. I’m sure it’s like, “Thank \n",
            "\n",
            "[Clayger goonsiant] I’m celling how he saying is the good confores. Now, I recome your bar. This is who shat they go, “Would a lect weird tign because everyone else think about your house very gon. They allowed to probably go in my life state soon, for the 10 is I don’t leave. And I went, “I think \n",
            "\n",
            "[Came chalberanh up on. Fuckething my boys don’t cost come bastard that you have to just leard to say son! [Audience cheering] Sa, there’s a woman getting together and the lady say, “Thank you that work everything?” Right now this. You’ve had to calm disabled and in Monday, right? In Austral kid hi\n",
            "\n",
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input (InputLayer)              [(None, 40)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 40, 100)      46500       input[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "rnn_1 (LSTM)                    (None, 40, 128)      117248      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "rnn_2 (LSTM)                    (None, 40, 128)      131584      rnn_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "rnn_concat (Concatenate)        (None, 40, 356)      0           embedding[0][0]                  \n",
            "                                                                 rnn_1[0][0]                      \n",
            "                                                                 rnn_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "attention (AttentionWeightedAve (None, 356)          356         rnn_concat[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "output (Dense)                  (None, 465)          166005      attention[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 461,693\n",
            "Trainable params: 461,693\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "9E8uhUFKw-Fh",
        "outputId": "d5bd1598-4f31-4271-d8a8-921a506efc8c"
      },
      "source": [
        "textgen_joe = textgenrnn(name = 'joerogan_model')\n",
        "textgen_joe.train_on_texts(joe_df_unclean,\n",
        "                            rnn_bidirectional = True,\n",
        "                            rnn_layers = 3,\n",
        "                            max_length = 40,\n",
        "                            rnn_size = 128,\n",
        "                            dim_embeddings = 300,\n",
        "                            num_epochs = 20,\n",
        "                            gen_epochs = 20)\n",
        "\n",
        "print(textgen_joe.model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/py_checkpoint_reader.py\u001b[0m in \u001b[0;36mNewCheckpointReader\u001b[0;34m(filepattern)\u001b[0m\n\u001b[1;32m     94\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mCheckpointReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m   \u001b[0;31m# TODO(b/143319754): Remove the RuntimeError casting logic once we resolve the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for joerogan_model",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-3b8470ff941d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtextgen_joe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtextgenrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'joerogan_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m textgen_joe.train_new_model(joe_df_unclean,\n\u001b[1;32m      3\u001b[0m                             \u001b[0mrnn_bidirectional\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                             \u001b[0mrnn_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                             \u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/textgenrnn/textgenrnn.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, weights_path, vocab_path, config_path, name, allow_growth)\u001b[0m\n\u001b[1;32m     79\u001b[0m         self.model = textgenrnn_model(self.num_classes,\n\u001b[1;32m     80\u001b[0m                                       \u001b[0mcfg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                                       weights_path=weights_path)\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices_char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/textgenrnn/model.py\u001b[0m in \u001b[0;36mtextgenrnn_model\u001b[0;34m(num_classes, cfg, context_size, weights_path, dropout, optimizer)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mweights_path\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[1;32m   2293\u001b[0m           'True when by_name is True.')\n\u001b[1;32m   2294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2295\u001b[0;31m     \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_detect_save_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2296\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msave_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tf'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2297\u001b[0m       \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trackable_saver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_detect_save_format\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m   2916\u001b[0m   \u001b[0;31m# directory. It's possible for filepath to be both a prefix and directory.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2917\u001b[0m   \u001b[0;31m# Prioritize checkpoint over SavedModel.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2918\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0m_is_readable_tf_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2919\u001b[0m     \u001b[0msave_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'tf'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2920\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0msm_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_is_readable_tf_checkpoint\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m   2937\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_is_readable_tf_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2938\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2939\u001b[0;31m     \u001b[0mpy_checkpoint_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNewCheckpointReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2940\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2941\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLossError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/py_checkpoint_reader.py\u001b[0m in \u001b[0;36mNewCheckpointReader\u001b[0;34m(filepattern)\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0;31m# issue with throwing python exceptions from C++.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0merror_translator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/py_checkpoint_reader.py\u001b[0m in \u001b[0;36merror_translator\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m     33\u001b[0m       \u001b[0;34m'Failed to find any '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m       'matching files for') in error_message:\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0merrors_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m   elif 'Sliced checkpoints are not supported' in error_message or (\n\u001b[1;32m     37\u001b[0m       \u001b[0;34m'Data type '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for joerogan_model"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uw-Fc-HzxEW7"
      },
      "source": [
        "textgen_john = textgenrnn(name = 'john_model')\n",
        "textgen_john.train_on_texts(john_df_unclean,\n",
        "                            rnn_bidirectional = True,\n",
        "                            rnn_layers = 3,\n",
        "                            max_length = 40,\n",
        "                            rnn_size = 128,\n",
        "                            dim_embeddings = 300,\n",
        "                            num_epochs = 20,\n",
        "                            gen_epochs = 20)\n",
        "\n",
        "print(textgen_john.model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eT9bW1UxKfx1"
      },
      "source": [
        "## Generating New Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taoYn1FqKfx1"
      },
      "source": [
        "textgen_ali = textgenrnn('aliwong_model_weights.hdf5')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YY5fJDGjKfx1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b10d2a64-3542-4d7b-f062-65a4ac686dcc"
      },
      "source": [
        "textgen_ali.generate(1,prefix='Today I', max_gen_length=150)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Today I was like, “You need to cleanse want to talk to you feel like you know? I don’t worry of hold on it– I’m just because you gotta shit the side \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0nnZFTiKfx2"
      },
      "source": [
        "textgen_hasan = textgenrnn('hasan_model_weights.hdf5')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjU76rQEKfx2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26aa578f-7fee-4bc7-9ca4-d0ec3461fbf4"
      },
      "source": [
        "textgen_hasan.generate(1, prefix = 'Today I', max_gen_length=150)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Today I realized the big shit really competne. I got this shit on your culture, but it’s not a letter than you got to go.” Then he says we pay to get\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMbTz7VaPmYz"
      },
      "source": [
        "textgen_bill = textgenrnn('billburr_model_weights (1).hdf5')"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBzpP4WYPxfU",
        "outputId": "b619b5cc-ca1e-4605-e8e0-2b5e6c9fd184"
      },
      "source": [
        "textgen_bill.generate(1, prefix='Today I', max_gen_length=150)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Today I got to be home in the movie. You got to fly a helicopter, but you got to stay the bot loaded on the whole though, all I’m gonna do today. I’m\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_k26VpiP1Y3"
      },
      "source": [
        "textgen_dave = textgenrnn('davechappelle_model_weights (1).hdf5')"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22sFuYd9QWyg",
        "outputId": "74a2b129-6a0b-4a92-9496-3454ebfbbea6"
      },
      "source": [
        "textgen_dave.generate(1, prefix = 'Today I', max_gen_length=150)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Today Iimiam. Here that stufe, and the wedch from every ghetto ♪ On Marco, but the fuck out that would be like it.” And I got to take what I said, “B\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BONkZSZyQcUL"
      },
      "source": [
        "textgen_jim = textgenrnn('jim_model_weights.hdf5')"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDjX54yDQnvO",
        "outputId": "3f9a699a-e226-4bdc-b14d-2f229511457a"
      },
      "source": [
        "textgen_jim.generate(1, prefix = 'Today I', max_gen_length=150)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Today I’m like, “Hey, why does my guy only happen in my actor and I just don’t know Jim Jefferies are you, “Well…” And I said, “All right. I go up to\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEOnYdkUQsDI"
      },
      "source": [
        "textgen_joe = textgenrnn('joerogan_model_weights.hdf5')"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiF_9ABDRbXd",
        "outputId": "5c2684bc-62da-4088-ef6b-4fe759cab40e"
      },
      "source": [
        "textgen_joe.generate(1, prefix = 'Today I', max_gen_length=150)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Today I’ve never been a lot of things that whole job, ’cause this white people in some respect! We get a problem with a problem with this? Is the Whi\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqR5SyWDRf-V"
      },
      "source": [
        "textgen_john = textgenrnn('john_model_weights.hdf5')"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ru43eKpjSAEy",
        "outputId": "f7fb6c83-0fe1-465c-d41d-7ffaf13b5618"
      },
      "source": [
        "textgen_john.generate(1, prefix = 'Today I', max_gen_length=150)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Today I get to mytinnving a river  years all the parents were like, “Thanks, it’s like, “Hey, I’m allegana. Hey! – Suremor Nichols were like… “Ah, th\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdSg1_D2Th3r"
      },
      "source": [
        "def joke_telling_robot(comedian, prefix, max_gen_length):\n",
        "  if comedian == 'Ali':\n",
        "    text = textgen_ali.generate(1, prefix = prefix, max_gen_length=max_gen_length)\n",
        "  if comedian == 'Dave':\n",
        "    text = textgen_dave.generate(1, prefix = prefix, max_gen_length=max_gen_length)\n",
        "  if comedian == 'Bill':\n",
        "    text = textgen_bill.generate(1, prefix = prefix, max_gen_length=max_gen_length)\n",
        "  if comedian == 'Hasan':\n",
        "    text = textgen_hasan.generate(1, prefix = prefix, max_gen_length=max_gen_length)\n",
        "  if comedian == 'Jim':\n",
        "    text = textgen_jim.generate(1, prefix = prefix, max_gen_length=max_gen_length)\n",
        "  if comedian == 'Joe':\n",
        "    text = textgen_joe.generate(1, prefix = prefix, max_gen_length=max_gen_length)\n",
        "  if comedian == 'John':\n",
        "    text = textgen_john.generate(1, prefix = prefix, max_gen_length=max_gen_length)\n",
        "  "
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7srFU4M0uKOt"
      },
      "source": [
        "def all_comedians(prefix, max_gen_length):\n",
        "  print('Ali Wong:') \n",
        "  textgen_ali.generate(1, prefix = prefix, max_gen_length=max_gen_length)\n",
        "  print('Dave Chappelle:')\n",
        "  textgen_dave.generate(1, prefix = prefix, max_gen_length=max_gen_length)\n",
        "  print('Bill Burr:')\n",
        "  textgen_bill.generate(1, prefix = prefix, max_gen_length=max_gen_length)\n",
        "  print('Hasan Minhaj:')\n",
        "  textgen_hasan.generate(1, prefix = prefix, max_gen_length=max_gen_length)\n",
        "  print('Jim Jeffries:')\n",
        "  textgen_jim.generate(1, prefix = prefix, max_gen_length=max_gen_length)\n",
        "  print('Joe Rogan:') \n",
        "  textgen_joe.generate(1, prefix = prefix, max_gen_length=max_gen_length)\n",
        "  print('John Mulaney:')\n",
        "  textgen_john.generate(1, prefix = prefix, max_gen_length=max_gen_length)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUVJZlYBu7ab",
        "outputId": "229b15f4-baa7-497f-c4be-cb06b8d13b7b"
      },
      "source": [
        "all_comedians('American politics are', 100)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ali Wong:\n",
            "American politics are your last year on our word. I just wanna leave his eyes and I am, have some u\n",
            "\n",
            "Dave Chappelle:\n",
            "American politics are that was the fuck up to a black dude that was coming out of that shit. They w\n",
            "\n",
            "Bill Burr:\n",
            "American politics are bums arsies. “You scare you, they fight it’s like a buly little offensive or \n",
            "\n",
            "Hasan Minhaj:\n",
            "American politics area. I didn’t know what I do. “Hasan Minhaj. You’ll need you!” “You’re all thing\n",
            "\n",
            "Jim Jeffries:\n",
            "American politics are allowed to see a lot of strip who see everyone else whole very here, you can’\n",
            "\n",
            "Joe Rogan:\n",
            "American politics are. That’s why the fuck when you watch you’re not getting sucg? ‘Cause he has ev\n",
            "\n",
            "John Mulaney:\n",
            "American politics are smoochiaaially should serve how three within she said, “Ah-he’d be real as a \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlxg5LHUxgso",
        "outputId": "f60813a9-398d-443a-a093-badc47c3fa09"
      },
      "source": [
        "all_comedians('Kids today are', 100)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ali Wong:\n",
            "Kids today are the look at the time, I do a quick shit in the baby. I don’t know what’s a big dad a\n",
            "\n",
            "Dave Chappelle:\n",
            "Kids today are your whole stars. I’d still watch a little bide. And then I wanted to have a high th\n",
            "\n",
            "Bill Burr:\n",
            "Kids today are ruined the street, they ask the shooter with the planes, out you this fucking religi\n",
            "\n",
            "Hasan Minhaj:\n",
            "Kids today are saying. I’m like, “Hasan bhainteen it’s back and if each other is secredied in this \n",
            "\n",
            "Jim Jeffries:\n",
            "Kids today are fucking keep with my guns. I deserve to earn as model. Now… See, me and the first th\n",
            "\n",
            "Joe Rogan:\n",
            "Kids today are men can don’t. Just get those shills just starts sleeping on?” I have a six-year-pla\n",
            "\n",
            "John Mulaney:\n",
            "Kids today are magtive estaien tcievilly. And we were like, “Well, I did a cubicle. And it would st\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C96XPhB4zziB",
        "outputId": "1c9b8fb2-734b-4ae9-db87-04783ff22303"
      },
      "source": [
        "all_comedians('The other day I', 100)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ali Wong:\n",
            "The other day I have the knicket. I meant had a nutramsial and it all fucked. That’s– the time in t\n",
            "\n",
            "Dave Chappelle:\n",
            "The other day I didn’t remember lady-hass you can imagine. I know him, when shit. I hold ten I didn\n",
            "\n",
            "Bill Burr:\n",
            "The other day I have the fucking leftoring about the movie. You hear to do that make sex sandwich. \n",
            "\n",
            "Hasan Minhaj:\n",
            "The other day I has ever seen this shit right now. I’m like, “I know the way it is as a great. You \n",
            "\n",
            "Jim Jeffries:\n",
            "The other day I can look at the locance that works out of you wanna pass.” And then they see each o\n",
            "\n",
            "Joe Rogan:\n",
            "The other day I can’t gotta really step. This is the world we get to make a kid trapperbul shit whe\n",
            "\n",
            "John Mulaney:\n",
            "The other day I’ get he really     you know who that would never walk in, it’s packer when and he w\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X14hYWMmV4Xi",
        "outputId": "7cc7f457-5b12-4a08-bfbd-51823cb6d1eb"
      },
      "source": [
        "joke_telling_robot('Ali', 'My Husband is', 200)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "My Husband is that this huge but if it ans as me from her staychare. That’s how all welcomethed and you gotta change it anything. So. I just wanna be a number in front of my company for a floo and I \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceFDl7LxV8hX",
        "outputId": "c0863402-98ea-4d00-c1eb-b20bbdf10171"
      },
      "source": [
        "joke_telling_robot('Bill', 'My life is', 200)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "My life is fucking leftore. Dude, he’s gonna get there when I was alrieved as your fored in 20 years, you go to pick your funning and all the sake of my wife. I’m like, “Where I don’t give a low in t\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVMl4JyeZ_Ip",
        "outputId": "f712f575-dbbc-4227-d7ba-b6da39058bf2"
      },
      "source": [
        "joke_telling_robot('Hasan', 'Politics are', 200)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Politics are around. I was like, “You know what kid US? You’re not an Indian girllib. “Well, you know whut you think you go to push. I’ll never make a school angrant the first little br0wn parents ar\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uksTHMbTaIBk",
        "outputId": "437f32d5-ff07-4f1a-d914-6bb1a7183e42"
      },
      "source": [
        "joke_telling_robot('Dave', 'People today are', 200)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "People today are paides to anything every ghetto ♪ ♪ Hollywood with her fuck up. They’re gonna go to stood up the fuck up. She doesn’t know the work to watch a white around the steps one than and had\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0039cMmibrR_",
        "outputId": "fadbfcff-f41e-4304-c878-9427bf568a0c"
      },
      "source": [
        "joke_telling_robot('Joe', 'In this world', 200)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "In this world we just starts really find that the calder. Every deas one of while the fuck are you ain’t the car, lase than anybody. The fuck are you ain’t the problem. We’re a girl in this fucking c\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rV6BdPevbz2W",
        "outputId": "a37613ac-daa1-48e9-d91e-1deba9597321"
      },
      "source": [
        "joke_telling_robot('John', 'My grandmother', 200)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "My grandmothers are marren try hour show that it? I really like having about her .chaned to self, that a nursery.” Why buy the cow all famlin before my wife was saying Mary. I am as a lone that way t\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U60GfVEib7ES"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}